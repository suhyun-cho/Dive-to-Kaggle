{"cells":[{"metadata":{},"cell_type":"markdown","source":"## A Simple Classification Challenge With LightGBM \n\nhttps://medium.com/@invest_gs/a-simple-classification-challenge-with-lightgbm-kaggle-competition-e12467cfec96\n\n#### a) target=1 데이터를 Over-Sampling 했을때 (Over-fitting될 수 있음)\n\n* Private Score : 0.27944\n* Public Score : 0.27624\n\n#### b) target=0 데이터를 Under-Sampling 했을때 (정보손실 될 수 있음)\n\n* Private Score : 0.27450\n* Public Score : 0.27004"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Load data sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\nval_df = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')\nsubmission_df=pd.read_csv('../input/porto-seguro-safe-driver-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load in\n\n# import numpy as np\n# import pandas as pd\n# import lightgbm\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelBinarizer\n\n\n# #\n# # Prepare the data\n# #\n\n# train = pd.read_csv('../input/train.csv')\n\n# # get the labels\n# y = train.target.values\n# train.drop(['id', 'target'], inplace=True, axis=1)\n\n# x = train.values\n\n# #\n# # Create training and validation sets\n# #\n# x, x_test, y, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n\n# #\n# # Create the LightGBM data containers\n# #\n# categorical_features = [c for c, col in enumerate(train.columns) if 'cat' in col]\n# train_data = lightgbm.Dataset(x, label=y, categorical_feature=categorical_features)\n# test_data = lightgbm.Dataset(x_test, label=y_test)\n\n\n# #\n# # Train the model\n# #\n\n# parameters = {\n#     'application': 'binary',\n#     'objective': 'binary',\n#     'metric': 'auc',\n#     'is_unbalance': 'true',\n#     'boosting': 'gbdt',\n#     'num_leaves': 31,\n#     'feature_fraction': 0.5,\n#     'bagging_fraction': 0.5,\n#     'bagging_freq': 20,\n#     'learning_rate': 0.05,\n#     'verbose': 0\n# }\n\n# model = lightgbm.train(parameters,\n#                        train_data,\n#                        valid_sets=test_data,\n#                        num_boost_round=5000,\n#                        early_stopping_rounds=100)\n# #\n# # Create a submission\n# #\n\n# submission = pd.read_csv('../input/test.csv')\n# ids = submission['id'].values\n# submission.drop('id', inplace=True, axis=1)\n\n\n# x = submission.values\n# y = model.predict(x)\n\n# output = pd.DataFrame({'id': ids, 'target': y})\n# output.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train, test데이터를 merge해서 한꺼번에 살펴본다\ncombined_df=pd.concat([train_df, val_df])\nprint(combined_df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Missing values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# target컬럼외에 null값이 있는 컬럼은 없음.\nprint(combined_df.columns[combined_df.isnull().any()])\n\n# 결측치 비율 확인 -> 없음\nprint(combined_df[[]].isnull().sum()/len(combined_df)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Data types\n* bin, cat으로 끝나는 컬럼은 categorical 타입."},{"metadata":{"trusted":true},"cell_type":"code","source":"# data type확인 : int타입이 48개, float타입이 11개\nfrom collections import Counter\nprint(Counter([combined_df[col].dtype for col in combined_df.columns.values.tolist()]).items())\n\n# ID 컬럼을 index로\ncombined_df.set_index('id',inplace=True)\n\n# categorical, binary값들은 더미변수 만들어주기 (for LGBM train remove it)\ncombined_df=pd.get_dummies(combined_df, columns=[col for col in combined_df if col.endswith('bin') or col.endswith('cat')])\n\n# 더미변수 만든 후 data type체크\nprint(Counter([combined_df[col].dtype for col in combined_df.columns.values.tolist()]).items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 4. Split data\n* train, validation set으로 데이터 분할"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combined_df를 다시 train_df, val_df로 나눈다\ntrain_df=combined_df.loc[combined_df['target'].isin([1,0])]\nval_df=combined_df[combined_df.index.isin(submission_df['id'])]\nval_df=val_df.drop(['target'],axis=1)\n\n# X_train_df, y_train_df를 만든다\nX_train_df=train_df.drop('target',axis=1)\ny_train_df=train_df['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Scaling the data\n*  RobustScaler, StandardScaler, MinMaxScaler 를 모두 적용해보고 score가 좋은것을 채택할것이다.\n    * 결과적으로 StandardScaler가 데이터에 가장 적합했다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# 각각의 scaler정의\n#scaler = RobustScaler()\n#scaler = MinMaxScaler()\nscaler=StandardScaler()\n\n# Scale the X_train set\nX_train_scaled=scaler.fit_transform(X_train_df.values)\nX_train_df=pd.DataFrame(X_train_scaled, index=X_train_df.index, columns=X_train_df.columns)\n\n# Scale the X_test set\nval_scaled=scaler.transform(val_df.values)\nval_df=pd.DataFrame(val_scaled, index=val_df.index, columns=val_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 6. Train test split\n* target값이 있는 train 데이터를 train,test로 나눠서 검증까지해보려고함\n    * 이 노트북에서 val_df는 target값이 없는 실제 우리가 예측할 x값들이 저장되어있고\n    * X_train은 모델 학습할 용도, X_test는 모델 검증용도임"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 20%를 검증용 데이터셋으로 구성해서 train, test분리\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X_train_df, y_train_df, test_size=0.2, random_state=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Class Imbalance\n* 클래스 불균형이 심한 데이터이므로 under sampling이나 over sampling을 해야하는데\n* 정보 손실을 막기위해 target=1인 값을 over sampling할것이다\n\n> 검증용 데이터(X_test, y_test)말고 훈련용 데이터(X_train, y_train)만 가지고 target=1인데이터 over sampling한것"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Upsample minority class\n\n# Concatenate our training data back together\nX=pd.concat([X_train, y_train],axis=1)\n\n# Separate minority and majority classes\nnot_target=X[X.target==0]\nyes_target=X[X.target==1]\nprint(not_target.shape)\nprint(yes_target.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### a) target=1인 데이터를 Over Sampling\n* target=0 개수 : 458,853\n* target=1 개수 : 17316 -> 458,853"},{"metadata":{"trusted":true},"cell_type":"code","source":"# target=1인 데이터를 target=0인 데이터만큼 oversampling한것 (=yes_target_up)\nfrom sklearn.utils import resample\nyes_target_up = resample(yes_target,\n                                replace = True, # sample without replacement\n                                n_samples = len(not_target), # match majority n\n                                random_state = 27) # reproducible results\n\n# Combine minority and downsampled majority\nupsampled=pd.concat([yes_target_up, not_target])\n\n# Checking counts (target=0과 target=1의 데이터개수가 동일해짐)\nprint(upsampled.target.value_counts())\n\n# Create training set again\nX_train=upsampled.drop('target',axis=1)\ny_train=upsampled.target\n\nprint(len(X_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b) target=0 인 데이터를 Under Sampling\ntarget=0인 데이터의 30%만 resample <br>\n\n* target=0 개수 : 458,853 -> 137,655\n* target=1 개수 : 17316 "},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.utils import resample\n# target_ratio=0.3  # target=0 데이터의 30% => 약 137,655 개\n# not_target_down = resample(not_target,\n#                                 replace = True, # sample without replacement\n#                                 n_samples = int(not_target.shape[0]*target_ratio), \n#                                 random_state = 27) # reproducible results\n\n# # Combine minority and downsampled majority\n# upsampled=pd.concat([yes_target, not_target_down])\n\n# # Checking counts (target=0과 target=1의 데이터개수가 동일해짐)\n# print(upsampled.target.value_counts())\n\n# # Create training set again\n# X_train=upsampled.drop('target',axis=1)\n# y_train=upsampled.target\n\n# print(len(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 여전히 검증용set에서는 target=1과 0의 비중은 다른 상태임!\n## 검증용set은 oversampling안하고 훈련용set만 oversampling해서 훈련할것\npd.concat([X_test,y_test],axis=1).target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Model and submission\n* LightGBM을 모델링을 할건데, 5000round 훈련시킬것임\n* overfitting을 방지하기위해 100round에서 stoppint"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LIGHT GBM\nimport lightgbm as lgbm\n\n# Indicate the categorical features for the LGBM classifier\n## 컬럼에 'cat'가 들어있는 컬럼의 위치 추출\ncategorical_features=[c for c, col in enumerate(X_train.columns) if 'cat' in col]\n\n# Get the train and test data for the training sequence\ntrain_data=lgbm.Dataset(X_train,label=y_train, categorical_feature=categorical_features)\ntest_data=lgbm.Dataset(X_test,label=y_test)\n\n# Set the parameters for training\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    #'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}\n\n# Train the classifier\nclassifier=lgbm.train(parameters, \n                      train_data, \n                      valid_sets=test_data,\n                     num_boost_round=5000,\n                     early_stopping_rounds=100)\n\n# Make predictions\npredictions=classifier.predict(val_df.values)\n\n# Create submission file\nmy_pred_lgbm=pd.DataFrame({'id':val_df.index,'target':predictions})\n\n# Create CSV file\nmy_pred_lgbm.to_csv('pred_lgbm_undersampling.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}